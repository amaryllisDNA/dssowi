swiss.pca$eig
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Outcome
var <- swiss.pca$var
names(var)
# * *Coordinates*: variables are represented by their correlations (same as factor analysis).
head(var$coord)
# * Plot "pc1" and "pc2" (first and second component/dimension)
fviz_pca_var(swiss.pca, col.var = "black")
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Run PCA
swiss.pca <- PCA(swiss.scale, # data
scale.unit = F, # z-scales (what we did manually, therefore turned off here)
ncp = 5, #number of dimensions kept in the final results.
graph = F) # if TRUE a graph is displayed.
# `swiss.pca` contains a lot of information:
print(swiss.pca)
# Most importantly: *Eigenvalues*
swiss.pca$eig
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Outcome
var <- swiss.pca$var
names(var)
# * *Coordinates*: variables are represented by their correlations (same as factor analysis).
head(var$coord)
# `swiss.pca` contains a lot of information:
print(swiss.pca)
# Most importantly: *Eigenvalues*
swiss.pca$eig
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Outcome
var <- swiss.pca$var
### Outcome
var <- swiss.pca$var
names(var)
# * *Coordinates*: variables are represented by their correlations (same as factor analysis).
head(var$coord)
### Run PCA
swiss.pca <- PCA(swiss.scale, # data
scale.unit = F, # z-scales (what we did manually, therefore turned off here)
ncp = 4, #number of dimensions kept in the final results.
graph = F) # if TRUE a graph is displayed.
# `swiss.pca` contains a lot of information:
print(swiss.pca)
# Most importantly: *Eigenvalues*
swiss.pca$eig
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Outcome
var <- swiss.pca$var
names(var)
# * *Coordinates*: variables are represented by their correlations (same as factor analysis).
head(var$coord)
### Run PCA
swiss.pca <- PCA(swiss.scale, # data
scale.unit = F, # z-scales (what we did manually, therefore turned off here)
ncp = 5, #number of dimensions kept in the final results.
graph = F) # if TRUE a graph is displayed.
# `swiss.pca` contains a lot of information:
print(swiss.pca)
# Most importantly: *Eigenvalues*
swiss.pca$eig
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Outcome
var <- swiss.pca$var
names(var)
# * *Coordinates*: variables are represented by their correlations (same as factor analysis).
head(var$coord)
# * Plot "pc1" and "pc2" (first and second component/dimension)
fviz_pca_var(swiss.pca, col.var = "black")
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
### Outcome
var <- swiss.pca$var
names(var)
names(var)
# * *Coordinates*: variables are represented by their correlations (same as factor analysis).
head(var$coord)
# * Plot "pc1" and "pc2" (first and second component/dimension)
fviz_pca_var(swiss.pca, col.var = "black")
head(var$cos2)
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)
# * color by cos2:
fviz_pca_var(swiss.pca,
axes = c(1,3), # which axes to plot
col.var = "cos2", # vars for colours
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # color scale [white,blue, red]
repel = TRUE # Avoid text overlapping
)
# * Plot "pc1" and "pc3"
fviz_pca_var(swiss.pca, axes = c(1, 3), col.var = "black")
# * color by cos2:
fviz_pca_var(swiss.pca,
axes = c(1,3), # which axes to plot
col.var = "cos2", # vars for colours
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # color scale [white,blue, red]
repel = TRUE # Avoid text overlapping
)
head(var$contrib)
head(var$contrib)
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)
# * Plot "pc1" and "pc3"
fviz_pca_var(swiss.pca, axes = c(1, 3), col.var = "black")
corrplot(var$cos2, is.corr=FALSE)
head(var$contrib)
head(var$contrib)
# * Plot by var (use "corrplot" again)
corrplot(var$contrib, is.corr=FALSE)
# * Plot as bar plot
fviz_contrib(swiss.pca, choice = "var", axes = 1, top = 10) # pc1
fviz_contrib(swiss.pca, choice = "var", axes = 2, top = 10) # pc2
# * Plot as bar plot
fviz_contrib(swiss.pca, choice = "var", axes = 1, top = 10) # pc1
fviz_contrib(swiss.pca, choice = "var", axes = 2, top = 10) # pc2
# * color by cos2:
fviz_pca_var(swiss.pca,
axes = c(1,3), # which axes to plot
col.var = "cos2", # vars for colours
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # color scale [white,blue, red]
repel = TRUE # Avoid text overlapping
)
# Often visualized as **scree plot**:
fviz_eig(swiss.pca,
addlabels = TRUE)
# * Plot as bar plot
fviz_contrib(swiss.pca, choice = "var", axes = 1, top = 10) # pc1
fviz_contrib(swiss.pca, choice = "var", axes = 2, top = 10) # pc2
pc.1 <- var$cor[order(-var$cor[, 'Dim.1']), 'Dim.1']
pc.2 <- var$cor[order(-var$cor[, 'Dim.2']), 'Dim.2'] # etc.
fviz_pca_ind(swiss.pca, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping (slow if many points)
)
# Similar to previous plots:
fviz_pca_ind(swiss.pca)
# Create list of individuals.
ind <- swiss.pca$ind
names(ind)
# That shows us which individuals/entities are associated with which variables. It is the metric version of correspondence analysis often used by, for instance, [Bourdieu](https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199357192.001.0001/oxfordhb-9780199357192-e-23).
biplot <- fviz_pca_biplot(swiss.pca, repel = TRUE,
col.var = "#2E9FDF", # Variables color
col.ind = "#696969"  # Individuals color
)
print(biplot)
# That shows us which individuals/entities are associated with which variables. It is the metric version of correspondence analysis often used by, for instance, [Bourdieu](https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199357192.001.0001/oxfordhb-9780199357192-e-23).
biplot <- fviz_pca_biplot(swiss.pca, repel = TRUE,
col.var = "red", # Variables color
col.ind = "#696969"  # Individuals color
)
print(biplot)
#### Loading data
data("USArrests")
df <- scale(USArrests) # produces type "matrix"! Doesn't matter in this case
head(df)
### Select K
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss") +    # total intra-cluster variation
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df, kmeans, method = "wss") +    # total intra-cluster variation
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df, kmeans, method = "wss") +    # total intra-cluster variation
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
fviz_nbclust(df, kmeans, nstart = 4,  method = "gap_stat", nboot = 10)+ # for "real" validation use 500 steps
geom_vline(xintercept = 4, linetype = 2) +
labs(subtitle = "Gap statistic method")
### Run K-MEANS
df.k <- kmeans(df, # data
4, # number of K
iter.max = 10, # number of iterations, default: 10
nstart = 2) # number of random sets,
print(df.k)
set.seed(123)
set.seed(123)
### Select K
set.seed(123)
random_numbers <- runif(5)  # Generate 5 random numbers
print(random_numbers)
set.seed(123)
random_numbers <- runif(5)  # Generate 5 random numbers
print(random_numbers)
random_numbers <- runif(5)  # Generate 5 random numbers
print(random_numbers)
set.seed(123)
random_numbers <- runif(5)  # Generate 5 random numbers
print(random_numbers)
set.seed(123)
random_numbers <- runif(5)  # Generate 5 random numbers
print(random_numbers)
set.seed(123)
random_numbers <- runif(5)  # Generate 5 random numbers
print(random_numbers)
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss") +    # total intra-cluster variation
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
fviz_nbclust(df, kmeans, nstart = 4,  method = "gap_stat", nboot = 10)+ # for "real" validation use 500 steps
geom_vline(xintercept = 4, linetype = 2) +
labs(subtitle = "Gap statistic method")
fviz_nbclust(df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
fviz_nbclust(df, kmeans, method = "wss") +    # total intra-cluster variation
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
fviz_nbclust(df, kmeans, nstart = 4,  method = "gap_stat", nboot = 10)+ # for "real" validation use 500 steps
geom_vline(xintercept = 4, linetype = 2) +
labs(subtitle = "Gap statistic method")
fviz_nbclust(df, kmeans, nstart = 2,  method = "gap_stat", nboot = 10)+ # for "real" validation use 500 steps
geom_vline(xintercept = 4, linetype = 2) +
labs(subtitle = "Gap statistic method")
fviz_nbclust(df, kmeans, nstart = 2,  method = "gap_stat", nboot = 10)+ # for "real" validation use 500 steps
geom_vline(xintercept = 4, linetype = 2) +
labs(subtitle = "Gap statistic method")
### Run K-MEANS
df.k <- kmeans(df, # data
4, # number of K
iter.max = 10, # number of iterations, default: 10
nstart = 2) # number of random sets,
print(df.k)
# Most importantly:
df.k$cluster
# Easy to assign to main df:
df <- data.frame(df)
df$cluster <- df.k$cluster
##### Visualize clusters
fviz_cluster(data = df,
df.k)
df.k$mean
df.cluster
df$cluster
# Check, for instance, mean of all clusters to explore substantial differences across clusters.
aggregate(df, by=list(cluster=df$cluster))
df.k$mean
# Check, for instance, mean of all clusters to explore substantial differences across clusters.
aggregate(df, by=list(cluster=df$cluster),mean)
df.k$mean
# Check, for instance, mean of all clusters to explore substantial differences across clusters.
aggregate(df, by=list(cluster=df$cluster),mean)
scan("https://www.projekt-gutenberg.org/zweig/nove-erz/chap003.html", what="character",sep"=")
scan("https://www.projekt-gutenberg.org/zweig/nove-erz/chap003.html", what="character",sep="")
<<#
<
for (pack in needed.packages){
if(pack %in% rownames(installed.packages())==FALSE)
{install.packages(pack)}
}
needed.packages<-c("factoextra","FactoMineR","datasets")
for (pack in needed.packages){
if(pack %in% rownames(installed.packages())==FALSE)
{install.packages(pack)}
}
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(here)
data("swiss")
names("swiss")
names(swiss)
head(swiss)
swiss.scale<-data.frame(scale(swiss))
summary(swiss.scale)
swiss.pca<-PCA(swiss.scale,scale.unit=F,ncp=5,graph=T)
print(swiss.pca)
fviz_eig(swiss.pca,addlabels=TRUE)
swiss.pca<-PCA(swiss.scale,scale.unit=F,ncp=3,graph=F)
fviz_eig(swiss.pca,addlables=TRUE)
fviz_eig(swiss.pca,addlables=TRUE)
swiss.pca<-PCA(swiss.scale,scale.unit=F,ncp=1,graph=F)
fviz_eig(swiss.pca,addlables=TRUE)
needed.packages<-c("factoextra","FactoMineR","datasets")
for (pack in needed.packages){
if(pack %in% rownames(installed.packages())==FALSE)
{install.packages(pack)}
}
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(here)
data("swiss")
names("swiss")
names(swiss)
head(swiss)
swiss.scale<-data.frame(scale(swiss))
summary(swiss.scale)
swiss.pca<-PCA(swiss.scale,scale.unit=F,ncp=5,graph=T)
print(swiss.pca)
fviz_eig(swiss.pca,addlabels=TRUE)
swiss.pca<-PCA(swiss.scale,scale.unit=F,ncp=1,graph=F)
var<-swiss.pca$var
names(var)
head(var$coord)
swiss.pca<-PCA(swiss.scale,scale.unit=F,ncp=5,graph=F)
fviz_eig(swiss.pca,addlables=TRUE)
var<-swiss.pca$var
names(var)
head(var$coord)
head(var$cor)
head(var$contriv)
head(var$contrib)
fviz_pca_var(swiss.pca,col.var="black")
fviz_pca_var(swiss.pca,axes=c(1,4),col.var="black")
head(var$cos2)
library(corrplot)
corrplot(var$cos2,is.corr=FALSE)
corrplot(var$cos2,is.corr=FALSE)
corrplot(var$cor,is.corr=TRUE)
corrplot(var$cor,is.corr=FALSE)
corrplot(var$contrib,is.corr=FALSE)
fviz_contrib(swiss.pca,choice="var",axes=1,top=10)
fviz_contrib(swiss.pca,choice="var",axes=2,top=10)
fviz_contrib(swiss.pca,choice="ind",axes=2,top=10)
pc.1<-var$cor[order(-var$cor[,"Dim.1"]),"Dim.1"]
pc.1
fviz_pca_ind(swiss.pca,col.ind="cos2",gradient.cols=c("blue","red","green",repel=TRUE))
fviz_pca_ind(swiss.pca,col.ind="cos2",gradient.cols=c("blue","red","green"),repel=TRUE))
fviz_pca_ind(swiss.pca,col.ind="cos2",gradient.cols=c("blue","red","green"),repel=TRUE)
biplot<-fviz_pca_biplot(swiss.pca,repel=TRUE,col.var="#2E9FDF",col.ind="#696969")
print(biplot)
data("USArrests")
df<-scale(USArrests)
head(df)
fviz_nbclust(df,kmeans,methode="wss")+
geom_vline(xintercepet=4,linetype=2)+
labs(subtitle="Elbow method")
fviz_nbclust(df,kmeans,methode="wss")+
geom_vline(xintercept=4,linetype=2)+
labs(subtitle="Elbow method")
fviz_nbclust(df,kmeans,method="wss")+
geom_vline(xintercept=4,linetype=2)+
labs(subtitle="Elbow method")
fviz_nbclust(df,kmeans,method="silhouette")+
corrplot(var$cos2,is.corr=FALSE)
fviz_nbclust(df,kmeans,method="silhouette")
fviz_clust(df,kmeans,nstart=4, method="gap_stat",nboot=10)+
geom_vline(xintercept=4,linetype=2)#
fviz_clust(df,kmeans,nstart=4, method="gap_stat",nboot=10)+
geom_vline(xintercept=4,linetype=2)+
labs(subtitle="Gap statistic method")
fviz_nbclust(df,kmeans,nstart=4, method="gap_stat",nboot=10)+
geom_vline(xintercept=4,linetype=2)+
labs(subtitle="Gap statistic method")
df.k<-kmeans(df,4,iter.max=10,nstart=2)
print(df.k)
df$cluster
df.k$cluster
df<-data.frame(df)
df$cluster<-df.k$cluster
fviz_cluster(data=df,df.k)
aggregare(df,by=list(cluster=df$cluster),mean)
aggregate(df,by=list(cluster=df$cluster),mean)
needed.packages<-c("factoextra","FactoMineR","datasets","ggplot2")
for (pack in needed.packages){}
needed.packages<-c("factoextra","FactoMineR","datasets","ggplot2")
for (pack in needed.packages){if(pack %in% rownames(installed.packages())==FALSE) {install.packages(pack)}}
library(factoextra)
library(FactoMineR)
library(datasets)
library(ggplot2)
data("swiss")
names(swiss)
head(swiss)
df<-as.data.frame(scale(swiss))
head(df)
fviz_nbclust(df.hcut,method="wss")+
geom_vline(xintercept=3,linetype=2)+
labs(subtitle="Elbow mehode")
fviz_nbclust(df,hcut,method="wss")+
geom_vline(xintercept=3,linetype=2)+
fviz_nbclust(df,hcut,method="wss")+
fviz_nbclust(df,hcut,method="wss")
fviz_nbclust(df,hcut,method="wss")+
fviz_nbclust(df,hcut,method="wss")+ geom_vline(xintercept=3,linetype=2)+
fviz_nbclust(df,hcut,method="wss")+ geom_vline(xintercept=3,linetype=2)
+labs(subtitle="Elbow method")
fviz_nbclust(df,hcut,method="wss")+ geom_vline(xintercept=3,linetype=2)+labs(subtitle="Elbow method")
fviz_nbclust(df,hcut,method="silhouette")+
fviz_nbclust(df,hcut,method="silhouette")+labs(subtitle="Silouette method")
d<-dist(df,methode="euclidean")
d<-dist(df,method="euclidean")
head(as.matrix(d))
hc.compl<-hclust(d,method="complete")
fviz_dent(hc.compl)
fviz_dend(hc.compl)
fviz_dend(hc.compl,3)
fviz_dens(hc.compl,2)
fviz_dend(hc.compl,2)
fviz_dend(hc.compl,4)
fviz_dend(hc.compl,4)
cluster<-cutree(hc.compl,4)
head(cluster)
df$cluster<-cluster
df$names<-row.names(df)
row.names(df)
head(df)
rownames(df)
ggplot(df,aes(x=Agriculture,y=Education,color=factor(cluster)))
ggplot(df,aes(x=Agriculture,y=Education,color=factor(cluster)))+geom_point()+geom_text(aes(label=nemas),size=2)
ggplot(df,aes(x=Agriculture,y=Education,color=factor(cluster)))+geom_point()+geom_text(aes(label=names),size=2)
metric.vars<-c("Fertility","Agriculture","Examination","Education","Catholic","Infant.Mortality")
cluster.descr<-aggregate(df[,metric.vars],by=list(cluster=df$cluster),mean)
cluster.descr
t(cluster.descr)
load("S:/Studium/DH/WinSe2425/Data Science für Sozialwiss/Data/.RData")
fitted.log<-fiited.log
fitted.log<-ifelse(fitted.log >0.5,1,0)
miss.classi<-mean(fitted.log !=testing$Churn)
print(paste("logistic Regression Accuracy",(1-round(miss.classi,4))))
print(paste("logistic Regression Accuracy",(1-round(miss.classi,4))))
testing$Churn
fitted.log
table(testing$Churn,fitted.log)
table(testing$Churn,fitted.log > 0.5)
load("S:/Studium/DH/WinSe2425/Data Science für Sozialwiss/Data/.RData")
load("S:/Studium/DH/WinSe2425/Data Science für Sozialwiss/Data/.RData")
coefs<-data.frame(Coef=coef(summary(model.log))[,1],Pvalue=coef(summary(model.log))[,4])
head(coefs[order(coefs$PValue),])
coefs<-data.frame(Coef=coef(summary(model.log))[,1],PValue=coef(summary(model.log))[,4])
head(coefs[order(coefs$PValue),])
library(partykit)
model.tree<-ctree(Churn~tenure+Contract+OnlineSecurity+TechSupport,data=training)
cahracter.var<-sapply(training,FUN=is.character)
for (i in names(training)[character.var]){}
for (i in names(training)[character.var]){training[,i]<-as.factoer(training[,i])}
for (i in names(training)[cahracter.var]){training[,i]<-as.factoer(training[,i])}
for (i in names(training)[cahracter.var]){training[,i]<-as.factor(training[,i])}
sapply(training, FUN=class)
cahracter.var
a<-names(training)
a
a[cahracter.var]
model.tree<-ctree(Churn~Contract+OnlineSecurity+tenure+MonthlyCharges,date=training)
model.tree<-ctree(Churn~Contract+OnlineSecurity+tenure+MonthlyCharges,data=training)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5),abbreviate=TRUE),pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,color="green"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,colour="green"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,colour="red"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="green"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="green",fill="darkgreen"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="green",fill=c("darkgreen"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="black",fill=c("darkgreen","lightgreen""),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="black",fill=c("darkgreen","lightgreen"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="black",fill=c("darkgreen","lightgreen"),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="black",fill=c("darkgreen","lightgreen")),abbreviate=TRUE,pval=F,id=FALSE)
plot(as.simpleparty(model.tree),gp=gpar(fontsize=5,col="black",fill=c("darkgreen","lightgreen")),abbreviate=TRUE,pval=F,id=FALSE)
character.var<-sapply(testing,FUN=is.character)
for(i in names(tesing)[character.var]){testing[,i]<-is.factor[tesing[,i]]}
for(i in names(testing)[character.var]){testing[,i]<-is.factor[testing[,i]]}
for(i in names(testing)[character.var]){testing[,i]<-is.factor(testing[,i])}
fitted.tree<-predict(model.tree,testing)
for(i in names(testing)[character.var]){testing[,i]<-as.factor(testing[,i])}
fitted.tree<-predict(testing,model.tree)
fitted.tree<-predict(model.tree,testing)
fitted.tree<-predict(model.tree,testing)
load("S:/Studium/DH/WinSe2425/Data Science für Sozialwiss/Data/.RData")
coefs<-data.frame(Coef=coef(summary(model.log))[,1],PValue=coef(summary(model.log))[,4])
libary(partykit)
library(partykit)
model.tree<-ctree(Churn~tenure+Contract+OnlineSecurity+TechSupport,data=training)
character.var<-sapply(training, FUN=is.character)
for(i in names(training)[character.var]){training[,i]<-as.factor(training[,i])}
model.tree<-ctree(Churn~tenure+Contract+OnlineSecurity+TechSupport,data=training)
for(i in names(testing)[character.var]){testing[,i]<-as.factor(testing[,i])}
fitted.tree<-predict(model.tree,testing)
print("Confusion Matrix for Decision Tree"); table(testing$Churn,fitted.tree>0.5)
tab<-table(Predicted=fitted.tree>0.5,Actual=testing$Churn)
print(paste("Decision Tree Acuracy",sum(diag(tab))/sum(tab)))
library(caret)
training$Churn<-as.factor(training$Churn)
testing$Churn<-as.factor(testing$Churn)
model.knn<-train(Churn~.,data=training,method="knn")
model.knn
coes.knn<varImp(model.knn)
coefs.knn<-varImp(model.knn)
plot(coefs.knn,main="Varible Importance with KNN")
training$TotalCharges<-Null
training$TotalCharges<-NULL
model.knn<-train(Churn~.,data=training,method="knn")
model.knn
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
getwd()
getwd()
setwd("S:/Studium/DH/WinSe2425/Data Science für Sozialwiss/Projekt/dssowi/Data")
setwd("S:/Studium/DH/WinSe2425/Data Science für Sozialwiss/Projekt/dssowi")
read.csv("ethn_zip.csv")
read.csv("ethn_zip.csv")
df<-read.csv("ethn_zip.csv")
names(df)
