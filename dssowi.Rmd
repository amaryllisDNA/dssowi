---
title: "DS_Project"
author: "Anthéa von Borowski, Mengjiao li"
date: "2024-12-09"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
#Thema und Forschungsfrage

#Nachhaltige Städte können nicht ohne städtische Wälder entwickelt werden. Stadtbäume, die Säulen der städtischen Wälder, spielen eine zentrale Rolle, indem sie unsere Gesundheit fördern, die Luft reinigen, Kohlendioxid speichern und die lokale Umgebung kühlen. Vor diesem Hintergrund haben wir uns für das Thema „Sozio-ökonomische Diskriminierung und Baumverteilung in US-Städten – am Beispiel New York City“ entschieden.

#Unsere Forschungsfrage lautet:
#„Wie spiegeln sich sozio-ökonomische Diskriminierungen in der Baumverteilung in US-Städten wider?“
#Hypothesen

##Stadtviertel mit höherem Einkommen weisen eine höhere Baumdichte auf.
##Stadtviertel mit höherem Einkommen haben eine größere Anzahl exotischer Baumarten.
##Stadtviertel mit einem hohen Anteil nicht-weißer Bevölkerungsgruppen (z. B. Latino oder Afroamerikaner) haben eine geringere Baumdichte.
##Stadtviertel mit einem hohen Anteil nicht-weißer Bevölkerungsgruppen verfügen über weniger exotische Baumarten.

#Daten

#Für die Untersuchung wurde ein umfassender und standardisierter Datensatz mit 5.660.237 Bäumen aus New York City zusammengestellt. Der Datensatz enthält detaillierte Informationen über:

##Baummerkmale: Standort, Baumart, Status der Einheimischen (einheimisch oder eingeführt), Gesundheitszustand, Größe.
##Umweltmerkmale: Unterscheidung zwischen Parks und urbanen Gebieten.

#Zusätzlich wurden weitere Datensätze integriert, die sozio-ökonomische und demografische Merkmale abdecken:

##Einkommensdaten der Stadtviertel.
##Demografische Informationen wie ethnische Zusammensetzung.
##Daten zur Stadtstruktur, z. B. Dichte von Wohn- und Geschäftsgebäuden.

#Die zugrunde liegenden Informationen stammen aus Baumbestandsaufnahmen, die auf Stadt- und Stadtteilebene durchgeführt wurden.

#Methodik

#Die Hypothesen werden mithilfe moderner Machine-Learning-Methoden überprüft:

##Supervised Learning:
###Lineare Regression: Zur Untersuchung der Beziehung zwischen Einkommen/Ethnizität und Baumdichte.
###Logistische Regression: Für die Analyse kategorischer Merkmale wie „exotische vs. einheimische Baumarten“.

##Unsupervised Learning:
###K-Means-Clustering: Zum Erkennen von Mustern und Clusterbildung in der Verteilung von Baumdichte und Baumarten auf Basis sozio-ökonomischer und ethnischer Variablen.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ressources

### Libraries

```{r lib, message=F}
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("readr")
#install.packages("olsrr")
#install.packages("lmtest")
#install.packages("moments")
install.packages("here")
install.packages("Rtools")
library(dplyr)
library(ggplot2)
library(readr)
library(lmtest)
library(olsrr)
library(moments)
library(here)
```

### Data Import

Data has been previously selected and modified in "data_import.Rmd".

```{r dataimport}

baumnyc<-read.csv(here("Data", "Refined", "baumnyc2.csv"))
baumnyc_gp<-read.csv(here("Data", "Refined", "baumnyc_gp2.csv"))

baumnyc$X<-NULL
baumnyc_gp$X<-NULL
```


### Multi-lineare Regression

#### Bäumedensität

H1.1 - Viertel mit höheren Einkommen haben mehr Baümedensität.
H2.1 - Viertel mit nicht-weissen/(Latino- Afroam?) Ethnien haben weniger Baümedensität.

```{r multireg1}

requiredh11<-c( "ZCTA", "mean_income", "eth_white_nonhisp", "eth_afroam_nonhisp", "eth_asian_nonhisp", "eth_hisp", "eth_other", "pop_density", "tree_density")


df_h11<-na.omit(baumnyc_gp[,requiredh11])
df_h11$income_category<-ifelse(df_h11$mean_income<=quantile(df_h11$mean_income, 0.25), 1, 
                               ifelse(df_h11$mean_income<=quantile(df_h11$mean_income, 0.5), 2,
                                      ifelse(df_h11$mean_income<=quantile(df_h11$mean_income, 0.75), 3, 4)))
#df_h11_log<-log(df_h11[-188,])
#df_h11_log<-df_h11_log[-39,]
#index(df_h11$ZCTA == 11005,)

regh11<-lm(tree_density~mean_income + eth_afroam_nonhisp + eth_asian_nonhisp + eth_hisp + eth_other + pop_density, df_h11)
summary(regh11)

summary(df_h11$tree_density)

``` 


```{r tests}

#OUTLIERS

ols_plot_resid_stud(regh11) #no values out of bounds
ols_plot_resid_lev(regh11)
ols_plot_cooksd_chart(regh11)

df_h11_no<-df_h11[-168,]
df_h11_no<-df_h11_no[-33,]

#LINEARITY - erfüllt 

plot(regh11)
pairs(df_h11)
raintest(regh11)

#MULTI-COLINEARITY NONE

#HETEROSKEDASTICITY

plot(regh11)

ols_plot_resid_fit(regh11)
ols_test_breusch_pagan(regh11)

#NORMALITY
ols_plot_resid_qq(regh11)
shapiro.test(regh11$residuals)
kurtosis(regh11$residuals)
skewness(regh11$residuals)
agostino.test(regh11$residuals)
anscombe.test(regh11$residuals)
```

#### Exotische Baüme

```{r multireg2}

requiredh12<-c( "ZCTA5CE10", "mean_income", "eth_white_nonhisp", "eth_afroam_nonhisp", "eth_asian_nonhisp", "eth_hisp", "eth_other", "pop_density", "native")


df_h12<-na.omit(baumnyc[,requiredh12])
df_h12$income_category<-ifelse(df_h12$mean_income<=quantile(df_h12$mean_income, 0.25), 1, 
                               ifelse(df_h12$mean_income<=quantile(df_h12$mean_income, 0.5), 2,
                                      ifelse(df_h12$mean_income<=quantile(df_h12$mean_income, 0.75), 3, 4)))
#df_h12_log<-log(df_h12[-188,])
#df_h12_log<-df_h11_log[-39,]
#index(df_h11$ZCTA == 11005,)

regh12<-glm(native~mean_income + eth_afroam_nonhisp + eth_asian_nonhisp + eth_hisp + eth_other + pop_density, df_h12, family="binomial")

plot(regh12)
summary(regh12)

summary(df_h11$tree_density)

``` 

```{r K-means clustering}

install.packages("jsonlite")
library(jsonlite)
library(here)
json_data <- fromJSON(here("Data", "Raw","zbp.json"))
data_frame <- as.data.frame(json_data, stringsAsFactors = FALSE)
write.csv(data_frame, "zbp.csv", row.names = FALSE)
library(here)

data1 <- read.csv(here("Data", "Raw", "zbp.csv"))
data2 <- read.csv(here("Data", "Refined", "Baumnyc_gp2.csv"))
names(data1)

data2$ESTAB <- NA

for (i in 1:nrow(data2)) {
  
  match_index <- which(data1$V2 == data2$ZCTA[i])
  
  
  if (length(match_index) > 0) {
    data2$ESTAB[i] <- data1$V1[match_index]
  }
}


write.csv(data2, here("Data", "Refined", "Baumnyc2_updated.csv"), row.names = FALSE)
fiele<-read.csv(here("Data","Refined","Baumnyc2_updated.csv"))
str(fiele)

```

``` {r}
library(ggplot2)
library(factoextra)
library(FactoMineR)
data_cluster<-read.csv(here("Data","Refined","Baumnyc2_updated.csv"))

data_cluster$ESTAB<-as.numeric(data_cluster$ESTAB)
data_cluster$ZCTA<-as.character(data_cluster$ZCTA)
data_cluster$ZCTA<-as.integer(data_cluster$ZCTA)
data_cluster$mean_income<-as.numeric(data_cluster$mean_income)
data_cluster$pop<-as.numeric(data_cluster$pop)
rownames(data_cluster)<-data_cluster$X
df<-scale(data_cluster[,c("ESTAB","eth_afroam_nonhisp","eth_asian_nonhisp","eth_hisp","eth_other","pop_density","tree_density","mean_income","surface","mean_native","mean_condition")])
summary(df)  # Übersicht der Daten
anyNA(df)    # Prüft, ob `NA`-Werte vorhanden sind
is.nan(df)   # Prüft auf `NaN`-Werte
is.infinite(df)  # Prüft auf `Inf`-Werte
df <- na.omit(df)
set.seed(123)


fviz_nbclust(df,kmeans,method="wss")+geom_vline(xintercept=4,linetype=2)+labs(title="Elbow methode")
```
# Nach Elbow Methode soll k == 3 oder 4 sein.  
```{r}
fviz_nbclust(df,kmeans,method="silhouette")+labs(subtitle="Silhouette method")
```
#Nach Silouhette Methode ist 8 ein guter Wert für K. 

```{r}
fviz_nbclust(df,kmeans,nstart=2,method="gap_stat",nboot=20)+
  geom_vline(xintercept=7,linetype=2)+
  labs(subtitle="Gap satistic method")
```
# Gap statistic methode emphehlt den Wert von 9.
# Dann schauen wir welcher K-Wert Sinn macht.
```{r}
df.k<-kmeans(df,3,iter.max=10,nstart=2)
print(df.k)
fviz_cluster(data=df,df.k)
```
```{r }
df.k<-kmeans(df,4,iter.max=10,nstart=2)
print(df.k)
fviz_cluster(data=df,df.k)
```

```{r }
df.k<-kmeans(df,9,iter.max=10,nstart=2)
print(df.k)
fviz_cluster(data=df,df.k)
```
# k=3 oder k= 4 sieht gut aus. K=7 ist schon schlecht. K=9 wird auf jeden Fall schlechter. 
# Deshalb 3 und 4 sind die besten Werte für k. Da wenn k=3, es weniger Überlappung gibt, ist 3 der beste Wert für K.Jetzt checken wir, mean von allen Clusters to 
```{r}
df.k<-kmeans(df,3,iter.max=10,nstart=2)
print(df.k)
df<-data.frame(df)
df$cluster<-df.k$cluster
```
```{r}
fviz_cluster(data=df,df.k)
```
#Wir checken jetzt Mittelwert aller Cluster, um wesentliche Unterschiede zwischen den Clustern zu untersuchen
```{r}
aggregate(df,by=list(cluster=df$cluster),mean)

```


#Literatur
#McCoy, Dakota; Goulet-Scott, Benjamin; Meng, Weilin et al. (2022). A dataset of 5 million city trees from 63 US cities: species, location, nativity status, health, and more. [Dataset]. Dryad. https://doi.org/10.5061/dryad.2jm63xsrf
#Burghardt KT, Avolio ML, Locke DH, Grove JM, Sonti NF, Swan CM. Current street tree communities reflect race-based housing policy and modern attempts to remedy environmental injustice. Ecology. 2023 Feb;104(2):e3881. doi: 10.1002/ecy.3881. Epub 2022 Dec 1. PMID: 36196604; PMCID: PMC10078568.
